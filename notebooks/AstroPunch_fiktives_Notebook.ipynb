{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7657197",
   "metadata": {},
   "source": [
    "\n",
    "Fiktives Datenaugmentierungs- & Trainings-Toolkit\n",
    "\n",
    "Dieses Notebook ist **vollst√§ndig fiktiv** und dient als **strukturell analoges Beispiel** zu einem ML-Repository mit Zeitreihenklassifikation.  \n",
    "Es zeigt eine plausible End-to-End-Pipeline von **README-√§hnlicher Einf√ºhrung** bis zu **Ergebnissen & Analyse** ‚Äî jedoch mit **frei erfundenen Daten, Code und Inhalten**.\n",
    "\n",
    
    "---\n",
    "\n",
    "## Inhaltsverzeichnis\n",
    "\n",
    "1. [README-√§hnliche Einf√ºhrung](#1)\n",
    "2. [Datasets‚ÄëBeschreibung (fiktiv)](#2)\n",
    "3. [Experimentelle Aufteilung](#3)\n",
    "4. [Modellaufbau](#4)\n",
    "5. [Training & Evaluation-Setup](#5)\n",
    "6. [Ergebnisse & Analyse](#6)\n",
    "7. [Anhang: Reproduzierbarkeit & Artefakte](#A)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c069f9f",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"1\"></a>\n",
    "\n",
    "## 1) README-√§hnliche Einf√ºhrung\n",
    "\n",
    "**AstroPunch** simuliert Sensordaten eines fiktiven Mars-Rovers und demonstriert, wie man:\n",
    "- ein **synthetisches Zeitreihen-Dataset** erzeugt,\n",
    "- **Augmentierungen** f√ºr robuste Modelle anwendet,\n",
    "- ein kleines **1D-CNN** in **PyTorch** trainiert,\n",
    "- eine **Experimentkonfiguration** verwaltet,\n",
    "- sowie **Metriken & Visualisierungen** erzeugt.\n",
    "\n",
    "### üéØ Ziele\n",
    "- Zeige eine klare, reproduzierbare Struktur f√ºr ML-Experimente mit Zeitreihen.\n",
    "- Mache die Pipeline (Daten ‚Üí Modell ‚Üí Training ‚Üí Auswertung) nachvollziehbar.\n",
    "- Bilde eine Vorlage ab, die man leicht an eigene Probleme anpassen kann.\n",
    "\n",
    "### ‚ö° Quickstart (lokal)\n",
    "```bash\n",
    "# (Optional) Neues Environment\n",
    "python -m venv .venv && source .venv/bin/activate  # Windows: .venv\\Scripts\\activate\n",
    "\n",
    "# Abh√§ngigkeiten (Beispiel)\n",
    "pip install numpy matplotlib scikit-learn torch torchvision torchaudio\n",
    "```\n",
    "\n",
    "> **Tipp:** GPU-Unterst√ºtzung f√ºr PyTorch ist optional. Das Notebook l√§uft auch auf CPU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b367cd7",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"2\"></a>\n",
    "\n",
    "## 2) Datasets‚ÄëBeschreibung (fiktiv)\n",
    "\n",
    "Wir simulieren **1D-Signale** (L√§nge `L=400` Samples) mit einer Abtastrate von `100 Hz`.  \n",
    "Jede Zeitreihe geh√∂rt zu genau **einer** der folgenden **4 Klassen** (fiktive Rover-Betriebszust√§nde):\n",
    "\n",
    "- `CRUISE`: regul√§re Fahrt ‚Äî glatte Sinusmuster mit geringer Varianz  \n",
    "- `DRILL`: Bohrbetrieb ‚Äî √ºberlagerte Periodik + sporadische Impulse  \n",
    "- `SCAN`: Scan-Modus ‚Äî modulierte Frequenzen (Chirp-√§hnlich)  \n",
    "- `FAULT`: St√∂rung ‚Äî unregelm√§√üige, verrauschte Muster\n",
    "\n",
    "### üì¶ Dateiformat & Struktur\n",
    "- F√ºr Illustration speichern wir Daten als `.npy`-Dateien mit einem `dict`:\n",
    "  - `{\"signal\": np.ndarray[L], \"label\": str, \"meta\": dict}`\n",
    "- Beispielhafte Ordner:\n",
    "```\n",
    "data/\n",
    " ‚îú‚îÄ‚îÄ train/\n",
    " ‚îú‚îÄ‚îÄ val/\n",
    " ‚îî‚îÄ‚îÄ test/\n",
    "```\n",
    "- **Beispiel-Metadaten (`meta`)**:\n",
    "  - `\"id\"`: eindeutige Sample-ID\n",
    "  - `\"sr\"`: sample rate in Hz (hier: 100)\n",
    "  - `\"created\"`: ISO-Zeitstempel der Generierung\n",
    "  - `\"generator\"`: kurzer Hinweis zur Signal-Engine\n",
    "\n",
    "> Das Dataset und alle Werte sind frei erfunden.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b45e384a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- Datengenerierung (synthetisch & fiktiv) ---\n",
    "from pathlib import Path\n",
    "import numpy as np, json, time, math\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "BASE = Path(\"data\")\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    (BASE / split).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SR = 100     # sample rate [Hz]\n",
    "L  = 400     # signal length [samples]\n",
    "CLASSES = [\"CRUISE\", \"DRILL\", \"SCAN\", \"FAULT\"]\n",
    "\n",
    "def gen_cruise(L):\n",
    "    t = np.linspace(0, L/SR, L, endpoint=False)\n",
    "    f = rng.uniform(0.5, 1.5)\n",
    "    sig = np.sin(2*np.pi*f*t) + rng.normal(0, 0.05, L)\n",
    "    return sig\n",
    "\n",
    "def gen_drill(L):\n",
    "    t = np.linspace(0, L/SR, L, endpoint=False)\n",
    "    f = rng.uniform(2.0, 4.0)\n",
    "    base = 0.8*np.sin(2*np.pi*f*t)\n",
    "    spikes = np.zeros(L)\n",
    "    for _ in range(rng.integers(3, 7)):\n",
    "        idx = rng.integers(0, L)\n",
    "        spikes[idx:idx+3] += rng.uniform(1.5, 2.5)\n",
    "    return base + spikes + rng.normal(0, 0.06, L)\n",
    "\n",
    "def gen_scan(L):\n",
    "    t = np.linspace(0, L/SR, L, endpoint=False)\n",
    "    f0, f1 = rng.uniform(0.2, 0.6), rng.uniform(1.0, 2.0)\n",
    "    phase = 2*np.pi*(f0*t + 0.5*(f1 - f0)*t**2/(t[-1] if t[-1] != 0 else 1))\n",
    "    return np.sin(phase) + rng.normal(0, 0.05, L)\n",
    "\n",
    "def gen_fault(L):\n",
    "    # Unregelm√§√üig, verrauscht, mit gelegentlichen Plateaus\n",
    "    sig = rng.normal(0, 0.3, L)\n",
    "    for _ in range(rng.integers(2, 5)):\n",
    "        a, b = sorted(rng.choice(np.arange(L), 2, replace=False))\n",
    "        sig[a:b] += rng.uniform(-1.0, 1.0)\n",
    "    return sig\n",
    "\n",
    "GEN = {\n",
    "    \"CRUISE\": gen_cruise,\n",
    "    \"DRILL\":  gen_drill,\n",
    "    \"SCAN\":   gen_scan,\n",
    "    \"FAULT\":  gen_fault,\n",
    "}\n",
    "\n",
    "def save_sample(path, signal, label, i):\n",
    "    meta = {\n",
    "        \"id\": f\"{label}_{i:05d}\",\n",
    "        \"sr\": SR,\n",
    "        \"created\": time.strftime(\"%Y-%m-%dT%H:%M:%SZ\", time.gmtime()),\n",
    "        \"generator\": \"AstroPunch v0 (fiktiv)\"\n",
    "    }\n",
    "    np.save(path, {\"signal\": signal.astype(np.float32), \"label\": label, \"meta\": meta}, allow_pickle=True)\n",
    "\n",
    "# Verteilungen pro Split (klein gehalten f√ºr Demo)\n",
    "N_TRAIN, N_VAL, N_TEST = 600, 200, 200\n",
    "per_class = lambda N: {c: N//len(CLASSES) for c in CLASSES}\n",
    "\n",
    "def build_split(split, N):\n",
    "    counts = per_class(N)\n",
    "    i = 0\n",
    "    for label, n in counts.items():\n",
    "        for _ in range(n):\n",
    "            sig = GEN[label](L)\n",
    "            save_sample(BASE/split/f\"{i:06d}.npy\", sig, label, i)\n",
    "            i += 1\n",
    "\n",
    "for split, N in [(\"train\", N_TRAIN), (\"val\", N_VAL), (\"test\", N_TEST)]:\n",
    "    # Erzeuge nur, wenn Ordner leer ist\n",
    "    if not any((BASE/split).iterdir()):\n",
    "        build_split(split, N)\n",
    "\n",
    "print(\"‚úì Synthetic dataset ready at ./data (fiktiv)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7aae17",
   "metadata": {},
   "source": [
    "\n",
    "### üîé Schneller Blick in die Daten\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f39a917",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np, matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "\n",
    "def load_any(split=\"train\"):\n",
    "    files = sorted((Path(\"data\")/split).glob(\"*.npy\"))\n",
    "    arr = []\n",
    "    for f in files[:8]:\n",
    "        d = np.load(f, allow_pickle=True).item()\n",
    "        arr.append(d)\n",
    "    return arr\n",
    "\n",
    "batch = load_any(\"train\")\n",
    "\n",
    "# 1 Plot pro Zelle (Regel), hier: erstes Beispiel\n",
    "plt.plot(batch[0][\"signal\"])\n",
    "plt.title(f\"Beispielsignal (train[0]) ‚Äî Label: {batch[0]['label']}\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e2ceec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Zweites Beispiel\n",
    "plt.plot(batch[1][\"signal\"])\n",
    "plt.title(f\"Beispielsignal (train[1]) ‚Äî Label: {batch[1]['label']}\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f3f1d7",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"3\"></a>\n",
    "\n",
    "## 3) Experimentelle Aufteilung\n",
    "\n",
    "### üìê Splits\n",
    "- **Train**: 600 Samples (‚âà 70 %)\n",
    "- **Val**: 200 Samples (‚âà 20 %)\n",
    "- **Test**: 200 Samples (‚âà 10 %)\n",
    "\n",
    "### üß™ Metriken\n",
    "- **Accuracy** (Top-1)\n",
    "- **Confusion Matrix** (Test)\n",
    "- Optional: **Precision/Recall/F1** pro Klasse (Bericht)\n",
    "\n",
    "### üîÅ Augmentierung (online, nur im Training)\n",
    "- **GaussianNoise**: Rauschen hinzuf√ºgen\n",
    "- **TimeShift**: zyklisches Verschieben\n",
    "- **Scale**: globale Amplitudenskalierung\n",
    "- **RandomDropout1D**: sporadisches Nullsetzen einiger Punkte\n",
    "\n",
    "### ‚ôªÔ∏è Reproduzierbarkeit\n",
    "- Fester Random-Seed f√ºr `numpy` und `torch`\n",
    "- Fixierte Hyperparameter in einer zentralen **Config**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b08e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, random, numpy as np, torch\n",
    "\n",
    "CONFIG = {\n",
    "    \"seed\": 2025,\n",
    "    \"data_dir\": \"data\",\n",
    "    \"classes\": [\"CRUISE\", \"DRILL\", \"SCAN\", \"FAULT\"],\n",
    "    \"sr\": 100,\n",
    "    \"length\": 400,\n",
    "    \"batch_size\": 64,\n",
    "    \"num_workers\": 0,\n",
    "    \"epochs\": 8,\n",
    "    \"lr\": 1e-3,\n",
    "    \"weight_decay\": 1e-4,\n",
    "    \"dropout\": 0.1,\n",
    "    \"augment\": {\n",
    "        \"gaussian_noise_std\": 0.05,\n",
    "        \"time_shift_max\": 20,\n",
    "        \"scale_min\": 0.9,\n",
    "        \"scale_max\": 1.1,\n",
    "        \"dropout_prob\": 0.02\n",
    "    }\n",
    "}\n",
    "\n",
    "def set_all_seeds(seed: int):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_all_seeds(CONFIG[\"seed\"])\n",
    "print(\"‚úì Seeds gesetzt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7416a5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import numpy as np, torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class GaussianNoise:\n",
    "    def __init__(self, std=0.05): self.std = std\n",
    "    def __call__(self, x):\n",
    "        return x + np.random.normal(0, self.std, size=x.shape)\n",
    "\n",
    "class TimeShift:\n",
    "    def __init__(self, max_shift=20): self.max_shift = max_shift\n",
    "    def __call__(self, x):\n",
    "        k = np.random.randint(-self.max_shift, self.max_shift+1)\n",
    "        return np.roll(x, k)\n",
    "\n",
    "class Scale:\n",
    "    def __init__(self, min_s=0.9, max_s=1.1): self.min_s, self.max_s = min_s, max_s\n",
    "    def __call__(self, x):\n",
    "        s = np.random.uniform(self.min_s, self.max_s)\n",
    "        return x * s\n",
    "\n",
    "class RandomDropout1D:\n",
    "    def __init__(self, p=0.02): self.p = p\n",
    "    def __call__(self, x):\n",
    "        mask = np.random.rand(*x.shape) > self.p\n",
    "        return x * mask\n",
    "\n",
    "class Compose:\n",
    "    def __init__(self, transforms): self.transforms = transforms\n",
    "    def __call__(self, x):\n",
    "        for t in self.transforms: x = t(x)\n",
    "        return x\n",
    "\n",
    "train_aug = Compose([\n",
    "    GaussianNoise(CONFIG[\"augment\"][\"gaussian_noise_std\"]),\n",
    "    TimeShift(CONFIG[\"augment\"][\"time_shift_max\"]),\n",
    "    Scale(CONFIG[\"augment\"][\"scale_min\"], CONFIG[\"augment\"][\"scale_max\"]),\n",
    "    RandomDropout1D(CONFIG[\"augment\"][\"dropout_prob\"]),\n",
    "])\n",
    "\n",
    "class AstroDataset(Dataset):\n",
    "    def __init__(self, root, split=\"train\", classes=None, length=400, augment=None):\n",
    "        self.root = Path(root)/split\n",
    "        self.files = sorted(self.root.glob(\"*.npy\"))\n",
    "        self.classes = classes or CONFIG[\"classes\"]\n",
    "        self.cls2idx = {c:i for i,c in enumerate(self.classes)}\n",
    "        self.length = length\n",
    "        self.augment = augment if split==\"train\" else None\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        d = np.load(self.files[idx], allow_pickle=True).item()\n",
    "        x = d[\"signal\"].astype(np.float32)\n",
    "        if x.shape[0] != self.length:\n",
    "            # pad/trim to fixed length if n√∂tig\n",
    "            if x.shape[0] > self.length:\n",
    "                x = x[:self.length]\n",
    "            else:\n",
    "                pad = self.length - x.shape[0]\n",
    "                x = np.pad(x, (0,pad))\n",
    "        if self.augment is not None:\n",
    "            x = self.augment(x)\n",
    "        y = self.cls2idx[d[\"label\"]]\n",
    "        # -> (C,L) f√ºr 1D-CNN\n",
    "        return torch.from_numpy(x).unsqueeze(0), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "train_ds = AstroDataset(CONFIG[\"data_dir\"], \"train\", CONFIG[\"classes\"], CONFIG[\"length\"], augment=train_aug)\n",
    "val_ds   = AstroDataset(CONFIG[\"data_dir\"], \"val\",   CONFIG[\"classes\"], CONFIG[\"length\"], augment=None)\n",
    "test_ds  = AstroDataset(CONFIG[\"data_dir\"], \"test\",  CONFIG[\"classes\"], CONFIG[\"length\"], augment=None)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CONFIG[\"batch_size\"], shuffle=True,  num_workers=CONFIG[\"num_workers\"])\n",
    "val_loader   = DataLoader(val_ds,   batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
    "test_loader  = DataLoader(test_ds,  batch_size=CONFIG[\"batch_size\"], shuffle=False, num_workers=CONFIG[\"num_workers\"])\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae6e3bf",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"4\"></a>\n",
    "\n",
    "## 4) Modellaufbau\n",
    "\n",
    "Wir verwenden ein kleines **1D-CNN** mit:\n",
    "- zwei Convolution-Bl√∂cken (Conv1d ‚Üí BatchNorm1d ‚Üí ReLU ‚Üí MaxPool1d),\n",
    "- **Dropout** zur Regularisierung,\n",
    "- einer **linearen** Klassifikationsschicht.\n",
    "\n",
    "Die Architektur ist bewusst kompakt gehalten, damit das Notebook z√ºgig l√§uft.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042f944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn, torch\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, c_in, c_out, k=5, p=2):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(c_in, c_out, kernel_size=k, padding=k//2),\n",
    "            nn.BatchNorm1d(c_out),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2, stride=2)\n",
    "        )\n",
    "    def forward(self, x): return self.block(x)\n",
    "\n",
    "class AstroCNN(nn.Module):\n",
    "    def __init__(self, n_classes=4, dropout=0.1, length=400):\n",
    "        super().__init__()\n",
    "        self.feat = nn.Sequential(\n",
    "            ConvBlock(1, 16),\n",
    "            ConvBlock(16, 32),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        # L√§nge reduzierter Feature-Map ermitteln\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1,1,length)\n",
    "            out = self.feat(dummy)\n",
    "            feat_len = out.shape[-1] * out.shape[1]\n",
    "        self.head = nn.Linear(feat_len, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.feat(x)\n",
    "        z = z.flatten(1)\n",
    "        return self.head(z)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = AstroCNN(n_classes=len(CONFIG[\"classes\"]), dropout=CONFIG[\"dropout\"], length=CONFIG[\"length\"]).to(device)\n",
    "model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411ab700",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"5\"></a>\n",
    "\n",
    "## 5) Training & Evaluation-Setup\n",
    "\n",
    "- **Optimierer:** Adam (`lr=1e-3`, `weight_decay=1e-4`)\n",
    "- **Kriterium:** CrossEntropyLoss\n",
    "- **Epochen:** 8 (f√ºr Demo; in echten Projekten erh√∂hen)\n",
    "- **Auswahl des besten Modells:** bestes **Val-Accuracy**\n",
    "\n",
    "Wir loggen **Loss** und **Accuracy** f√ºr Train/Val und visualisieren sie anschlie√üend.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c1c5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict\n",
    "import numpy as np, torch, math, os\n",
    "\n",
    "optimizer = Adam(model.parameters(), lr=CONFIG[\"lr\"], weight_decay=CONFIG[\"weight_decay\"])\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "def run_epoch(loader, train: bool):\n",
    "    model.train(mode=train)\n",
    "    total, correct = 0, 0\n",
    "    running_loss = 0.0\n",
    "    for x,y in loader:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        running_loss += loss.item() * x.size(0)\n",
    "        pred = logits.argmax(dim=1)\n",
    "        total += y.size(0)\n",
    "        correct += (pred == y).sum().item()\n",
    "    return running_loss/total, correct/total\n",
    "\n",
    "history = defaultdict(list)\n",
    "best_val, best_state = -1.0, None\n",
    "\n",
    "for epoch in range(1, CONFIG[\"epochs\"]+1):\n",
    "    tr_loss, tr_acc = run_epoch(train_loader, True)\n",
    "    va_loss, va_acc = run_epoch(val_loader, False)\n",
    "\n",
    "    history[\"train_loss\"].append(tr_loss)\n",
    "    history[\"train_acc\"].append(tr_acc)\n",
    "    history[\"val_loss\"].append(va_loss)\n",
    "    history[\"val_acc\"].append(va_acc)\n",
    "\n",
    "    if va_acc > best_val:\n",
    "        best_val = va_acc\n",
    "        best_state = {k:v.cpu() for k,v in model.state_dict().items()}\n",
    "\n",
    "    print(f\"[{epoch:02d}/{CONFIG['epochs']}]  \"\n",
    "          f\"train_loss={tr_loss:.4f}  train_acc={tr_acc:.3f}  |  \"\n",
    "          f\"val_loss={va_loss:.4f}  val_acc={va_acc:.3f}\")\n",
    "\n",
    "# Lade bestes Val-Modell\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "    print(f\"‚úì Bestes Val-Accuracy: {best_val:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399aad3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history[\"train_loss\"])\n",
    "plt.plot(history[\"val_loss\"])\n",
    "plt.title(\"Loss √ºber Epochen\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5de3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(history[\"train_acc\"])\n",
    "plt.plot(history[\"val_acc\"])\n",
    "plt.title(\"Accuracy √ºber Epochen\")\n",
    "plt.xlabel(\"Epoche\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend([\"Train\", \"Val\"])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b1a67d",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"6\"></a>\n",
    "\n",
    "## 6) Ergebnisse & Analyse\n",
    "\n",
    "Wir bewerten das **beste** Validierungsmodell auf dem **Test**-Split und betrachten:\n",
    "- Accuracy\n",
    "- Confusion Matrix\n",
    "- Klassifikationsbericht (Precision/Recall/F1)\n",
    "- Beispielvorhersagen (Qualitativ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a9303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import numpy as np, torch\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x,y in test_loader:\n",
    "        x = x.to(device)\n",
    "        logits = model(x)\n",
    "        pred = logits.argmax(dim=1).cpu().numpy().tolist()\n",
    "        y_pred.extend(pred)\n",
    "        y_true.extend(y.numpy().tolist())\n",
    "\n",
    "test_acc = (np.array(y_true) == np.array(y_pred)).mean()\n",
    "print(f\"Test-Accuracy: {test_acc:.3f}\")\n",
    "\n",
    "# Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=list(range(len(CONFIG[\"classes\"]))))\n",
    "cm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd02cba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualisierung der Confusion Matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title(\"Confusion Matrix (Test)\")\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(CONFIG[\"classes\"]))\n",
    "plt.xticks(tick_marks, CONFIG[\"classes\"], rotation=45)\n",
    "plt.yticks(tick_marks, CONFIG[\"classes\"])\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b83660f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Klassifikationsbericht (Text)\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred, target_names=CONFIG[\"classes\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32928bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Qualitative Beispiele: Plot einiger Test-Signale mit Vorhersage\n",
    "import numpy as np, matplotlib.pyplot as plt, torch, random\n",
    "\n",
    "indices = random.sample(range(len(test_ds)), k=min(4, len(test_ds)))\n",
    "for idx in indices:\n",
    "    x, y = test_ds[idx]\n",
    "    with torch.no_grad():\n",
    "        logits = model(x.unsqueeze(0).to(device))\n",
    "        pred = int(logits.argmax(dim=1).cpu().item())\n",
    "    plt.plot(x.squeeze(0).numpy())\n",
    "    plt.title(f\"True: {CONFIG['classes'][y]} | Pred: {CONFIG['classes'][pred]}\")\n",
    "    plt.xlabel(\"Samples\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e05fdb",
   "metadata": {},
   "source": [
    "\n",
    "<a id=\"A\"></a>\n",
    "\n",
    "## Anhang: Reproduzierbarkeit & Artefakte\n",
    "\n",
    "- **Config** & **Metriken** werden gespeichert.\n",
    "- **Modellgewichte** (`.pt`) und Statistiken (`.json`) erleichtern sp√§tere Auswertungen.\n",
    "- Zus√§tzlich werden **Bibliotheksversionen** dokumentiert.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01cfe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import json, pandas as pd, torch\n",
    "\n",
    "out_dir = Path(\"artifacts\")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# 1) Config speichern\n",
    "with open(out_dir/\"config.json\", \"w\") as f:\n",
    "    json.dump(CONFIG, f, indent=2)\n",
    "\n",
    "# 2) Training-Historie als CSV\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "hist_df = pd.DataFrame({\n",
    "    \"epoch\": np.arange(1, len(history[\"train_loss\"])+1),\n",
    "    \"train_loss\": history[\"train_loss\"],\n",
    "    \"train_acc\": history[\"train_acc\"],\n",
    "    \"val_loss\": history[\"val_loss\"],\n",
    "    \"val_acc\": history[\"val_acc\"]\n",
    "})\n",
    "hist_df.to_csv(out_dir/\"history.csv\", index=False)\n",
    "\n",
    "# 3) Bestes Modell speichern\n",
    "torch.save(model.state_dict(), out_dir/\"best_model.pt\")\n",
    "\n",
    "# 4) Confusion Matrix & Test-Accuracy\n",
    "np.save(out_dir/\"confusion_matrix.npy\", cm)\n",
    "with open(out_dir/\"test_metrics.json\", \"w\") as f:\n",
    "    json.dump({\"test_accuracy\": float((np.array(cm).trace()/np.array(cm).sum()) if cm.sum()>0 else 0.0)}, f, indent=2)\n",
    "\n",
    "print(\"‚úì Artefakte gespeichert in ./artifacts\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb6189a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Bibliotheksversionen\n",
    "import sys, numpy, sklearn, torch, matplotlib\n",
    "print(\"Python :\", sys.version.split()[0])\n",
    "print(\"NumPy  :\", numpy.__version__)\n",
    "print(\"PyTorch:\", torch.__version__)\n",
    "print(\"sklearn:\", sklearn.__version__)\n",
    "print(\"mpl    :\", matplotlib.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b98601c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Zusammenfassung\n",
    "- Vollst√§ndige, strukturierte Pipeline (Daten ‚Üí Augmentierung ‚Üí Modell ‚Üí Training ‚Üí Auswertung).\n",
    "- Alles frei erfunden und unabh√§ngig von bestehenden Projekten.\n",
    "- Kann als **Vorlage** f√ºr eigene Zeitreihen-Experimente dienen.\n",
    "\n",
    "Viel Erfolg beim Anpassen! üßë‚ÄçüöÄ\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
